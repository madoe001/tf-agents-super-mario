{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Gym Retro.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fY4jXUaEDvie"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madoe001/tf-agents-super-mario/blob/main/Gym_Retro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QFeVIXIw8qf"
      },
      "source": [
        "# Trainieren eines Reinforcement Learning Models mit TF-Agents, welches Super Mario spielen kann\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_uaHQdsOwAx"
      },
      "source": [
        "## 1. Vorwort\n",
        "Im Rahmen der Modulsprüfung zum Modul \"Machine Learning\" des  Verbundsstudiengangs \"Angewandte Informatik\" der Fachhochschule Südwestfalen, ist eine Ausarbeitung anzufertigen, welche sich thematisch mit dem Erstellen von Machine Learning-Modellen auseinandersetzt. \n",
        "\n",
        "Dazu wurden von den jeweiligen Studierenden, Projektvorschläge vorgelegt. Die vorliegende Arbeit, setzt sich mit der Anwendung von Reinforcement Learning-Algorithmen auseinander. Als Anwendungsszenario wurde das Erstellen eines Models, welches in der Lage ist das Nintendo Spiel \"Super Mario Bros.\", gewählt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMOGnn5TCD4X"
      },
      "source": [
        "## 2. Projektbeschreibung\n",
        "Das Ziel des Projekts ist die Entwicklung eines Machine Learning Models, welches eigenständig lernt \"Super Mario Bros.\" zu spielen. Die Umsetzung dieses Models erfolgt mit Hilfe der Machine Learning Bibliothek Tensorflow, Tensorflow-Agents und Open-AI Gym Retro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XxS9GSZd8SR"
      },
      "source": [
        "## TensorFlow-Agents\n",
        "\n",
        "[Quelle](https://www.tensorflow.org/agents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pPUn0pXd-eB"
      },
      "source": [
        "## OpenAI-Gym / OpenAI-Gym Retro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0HlrokLeCww"
      },
      "source": [
        "## Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zTCR-NPeFBq"
      },
      "source": [
        "## Deep Q-Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIF5wiV-eH2z"
      },
      "source": [
        "## Policy Gradient\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFPS8f4xeMP6"
      },
      "source": [
        "## Proximal Policy Optimization (PPO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9e0Mi2tM_vv"
      },
      "source": [
        "# 3. Reinforcement Learning\n",
        "Reinfocrcement Learning ist eine Disziplin des Machine Learnings. Mit ihr ist es möglich ein Model zu trainieren, welches in der Lage ist selbstständig Aufgaben zu lösen. Das zu trainierende Model heißt *Agent*. Dieser beobachtet eine *Umgebung*, wie zum Beispiel eine Spielwelt. Der Agent kann mit Hilfe von *Aktionen* mit dieser Spielwelt interagieren. Eine Aktion ist zum Beispiel eine Seitwärtsbewegung einer Spielfigur oder ein Zug im Schach. Das Model erhält nach der Durchführung eine Belohnung und eine Beobachtung. Die Belohnung, auch Reward genannt, dient dazu die Interaktion des Agents mit der Umgebung zu bewerten. Ziel des Models ist es anhand der Belohnungen die beste Strategie zu entwickeln um das gegebene Problem zu lösen. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25ZqdBwyNJDk"
      },
      "source": [
        "## 4. Super Mario Bros.\n",
        "\n",
        "[Quelle](https://de.wikipedia.org/wiki/Super_Mario_Bros.)\n",
        "\n",
        "Super Mario Bros. ist ein Jump-'n'-Run Videospiel, der Firma Nintendo. Der Spieler übernimmt in dem Spiel die Rolle des Klempners Mario. Mit der Figur Mario, muss der Spieler anschließend Hindernisse überwinden und Gegner besiegen. Die Spielwelt ist zweidimensional und beinhaltet verschiedene Level mit unterschiedlichen Schwierigkeitsgraden. Ziel eines Levels ist es das Schloss am Ende einese Levels zu erreichen. Die Spielfigur bewegt sich dazu von der linken Seite eines Levels zur rechten Seite. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrRzda65P3to"
      },
      "source": [
        "### Aktionen:\n",
        "Zur Steuerung der Spielfigur \"Mario\" stehen dem Spieler folgende Aktionen zur Verfügung. Diese können beliebig miteinander kombiniert werden, wodurch der Aktionsraum verhältnismäßig groß wird.\n",
        "\n",
        "*   **Pfeiltaste Links/Rechts:** <br/>\n",
        " * Bewegt die Figur nach links oder Rechts\n",
        "\n",
        "*   **Pfeiltaste Unten:** <br/>\n",
        " * Durch drücken der Pfeiltaste unten, duckt sich Mario. \n",
        " * Steht Mario zu dem  Zeitpunkt auf einer Röhre, kann er diese, die Röhre betreten.\n",
        " * Runterklettern von Ranken\n",
        " \n",
        "* **Pfeiltaste Unten:** <br/>\n",
        " * Raufklettern von Ranken\n",
        "\n",
        "*   **A-Knopf:** <br/>\n",
        " * Durch drücken des A-Knopfs springt Mario.\n",
        "\n",
        "*   **B-Knopf:** <br/>\n",
        " * Durch drücken des B-Knopfs sprintet Mario und kann gegebenenfalls Feuerbälle werfen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNH4_hL6PvB6"
      },
      "source": [
        "### Umgebung:\n",
        "Die Umgebung ist eine Open-AI Gym Retro Umgebung, die mit Hilfe eines Emulators das Spiel rendert. Das Spiel \"Super Mario Bros.\" ist eine zweidimensionale Spielwelt. Insgesamt besteht sie aus 224 x 240 Pixeln. Pro Pixel sind drei Werte angegeben, die den jeweiligen Farbkanal (RGB) codieren. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeb5KBqbQJgH"
      },
      "source": [
        "### Belohnung\n",
        "Die Belohnung, welche durch die Open-AI Gym Retro bereitgestellt wird, ist die Bewegung des Bildschirm nach rechts. Umso weiter der Agent es schafft in dem Level nach rechts zu kommen, desto höher ist die Belohnung. Das Level ist absolviert, sobald \"Super Mario\" im Spiel das Schloss erreicht hat. Die Umgebung signalisiert dies mit dem flag *done*. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baMU5MgxQLvN"
      },
      "source": [
        "### Beobachtung\n",
        "Als Beobachtung wird ein Screenshot des aktuellen Zustands der Spielwelt zurückgeliefert. Bei Super Mario ist dies ein Array in der Form 224 x 240 x 3. Dieses beinhaltet 224 x 240 Pixel, welche durch drei Farbwerte codiert werden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a35N0B2dQT4_"
      },
      "source": [
        "# 5. Projekt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD5Isf8fxnQL"
      },
      "source": [
        "Installieren der benötigten Abhängigkeiten"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPgnEEzF-fMH",
        "outputId": "ae581bed-8d06-4e42-a8d2-bb273edb9268"
      },
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y xvfb ffmpeg > /dev/null\n",
        "!pip install 'imageio==2.4.0' > /dev/null\n",
        "!pip install pyvirtualdisplay > /dev/null\n",
        "!pip install tf-agents > /dev/null\n",
        "!pip install gym-retro > /dev/null\n",
        "!pip install git+https://github.com/openai/baselines.git > /dev/null\n",
        "!pip install retrowrapper > /dev/null"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Waiting for headers] [Co\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\r0% [Waiting for headers] [3 InRelease 14.2 kB/88.7 kB 16%] [Waiting for headers\r                                                                               \rIgn:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [3 InRelease 14.2 kB/88.7 kB 16%] [Waiting for headers\r                                                                               \rHit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Fetched 252 kB in 1s (215 kB/s)\n",
            "Reading package lists... Done\n",
            "\u001b[31mERROR: baselines 0.1.6 has requirement gym<0.16.0,>=0.15.4, but you'll have gym 0.18.3 which is incompatible.\u001b[0m\n",
            "  Running command git clone -q https://github.com/openai/baselines.git /tmp/pip-req-build-kjrrdth0\n",
            "\u001b[31mERROR: tf-agents 0.8.0 has requirement gym>=0.17.0, but you'll have gym 0.15.7 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: gym 0.15.7 has requirement cloudpickle~=1.2.0, but you'll have cloudpickle 1.6.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tf-agents 0.8.0 has requirement cloudpickle>=1.3, but you'll have cloudpickle 1.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tf-agents 0.8.0 has requirement gym>=0.17.0, but you'll have gym 0.15.7 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.2 has requirement cloudpickle>=1.3, but you'll have cloudpickle 1.2.2 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34UUPc1YKKcT"
      },
      "source": [
        "Mounten eines Google-Drive Laufwerks um dem Notebook die ROM für Super Mario zur Verfügung zu stellen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoeaC-sqsT5j",
        "outputId": "16840f32-8045-474e-e1d9-e4915f26772e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dyxchq5KTOW"
      },
      "source": [
        "Importieren der benötigten Abhängikeiten."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBakLhc8-fMI"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent \n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import utils\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.utils import common"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-eOf8qvKYcE"
      },
      "source": [
        "Erstellen eines virtuellen Displays, um der Bibliothek eine Ausgabe des Bildes zu simulieren"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO5HswnV-fMI"
      },
      "source": [
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BX2S39KKgJH"
      },
      "source": [
        "Importieren der \"Super Mario Bros.\"-ROM mit Hilfe des von OpenAI-Gym Retro bereitgestellten Skripts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lVZ4WKW4DBn",
        "outputId": "a351b898-551f-40e5-b6a8-bb665ed50505"
      },
      "source": [
        "!python3 -m retro.import \"/content/drive/MyDrive/ROMS/\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Importing SuperMarioBros-Nes\n",
            "Imported 1 games\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYWsPTtyK3Hi"
      },
      "source": [
        "Validieren, dass die OpenAI-Gym Umgebung korrekt funktioniert."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOaQq_mI-fMJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "24934fa8-f4cb-4423-f2cf-8e6a26151b21"
      },
      "source": [
        "import retro\n",
        "env = retro.make(game='SuperMarioBros-Nes')\n",
        "env.reset()\n",
        "env.step(env.action_space.sample())\n",
        "img = PIL.Image.fromarray(env.render(mode=\"rgb_array\"))\n",
        "img"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=240x224 at 0x7F17716E2E90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XD770EnDd1q"
      },
      "source": [
        "## Hilfsfunktionen definieren\n",
        "Die folgenden Zellen erstellen ein paar Hilfsfunktionen, welche im Laufe des Projekts genutzt wurden. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fY4jXUaEDvie"
      },
      "source": [
        "#### Combine\n",
        "Die Funktion führt zwei Arrays, welche nur aus Nullen und Einsen bestehen zusammen. Die Einsen von beiden Arrays werden in ein neues Array übernommen. Die Funktion wird im späteren Verlaufe genutzt, um die Aktionen von lesbaren Texten wie zum Beispiel `\"RIGHT\"` in Aktionsarrays `[0, 0, 0, 0, 0, 0, 0, 1, 0]` zu überführen. Das erhaltene Array kann genutzt werden um einen weiteren Schritt in der Umgebung auszuführen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2u5JxOw5fZU"
      },
      "source": [
        "def takeOneValues(source, target): \n",
        "  for idx, value in enumerate(source):\n",
        "    if value == 1: \n",
        "      target[idx] = 1\n",
        "  return target\n",
        "\n",
        "def combine(list1, list2): \n",
        "  if len(list1) > len(list2): \n",
        "    size = len(list1)\n",
        "  else:\n",
        "    size = len(list2)\n",
        "  newList = [0]*size\n",
        "  takeOneValues(list1, newList)\n",
        "  takeOneValues(list2, newList)\n",
        "  return newList\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77mr-9edE-I6"
      },
      "source": [
        "#### Überführen der Aktionen in ein Dictionary\n",
        "Als nächstes wird ein Dictionary erzeugt in dem alle möglichen Aktionen, die in der \"Super Mario Bros.\" Umgebung ausführbar sind, gespeichert werden. Als Key des Dictionaries dient die Text-Repräsentation der Aktionen. Ein Array der Größe 9, welches aus Nullen und Einsen besteht und die Aktion für die Umgebung codiert, bildet den Value. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8n8C1e2y6VhQ",
        "outputId": "be347c2a-42a2-46d3-ed1f-add0b05ac991"
      },
      "source": [
        "single_actions = {} \n",
        "\n",
        "for index in range(9):\n",
        "  action = [0]*9\n",
        "  action[index] = 1\n",
        "  key = env.get_action_meaning(action)\n",
        "  if len(key) >= 1: key = key[0]\n",
        "  else: continue\n",
        "  single_actions[key] = action\n",
        "\n",
        "\n",
        "single_actions"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'A': [0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              " 'B': [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " 'DOWN': [0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
              " 'LEFT': [0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
              " 'RIGHT': [0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
              " 'UP': [0, 0, 0, 0, 1, 0, 0, 0, 0]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qXkNDLOGGfp"
      },
      "source": [
        "### Definieren der möglichen Aktionen\n",
        "Wie oben beschrieben, können Aktionen beliebig miteinander kombiniert werden. Die möglichen Aktionen, die im Spiel \"Super Mario Bros.\" nutzbar sind, werden im Folgenden definiert.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2te1I3x13FR"
      },
      "source": [
        "actions = [\n",
        "    ['A'],\n",
        "    ['B'],\n",
        "    ['RIGHT', 'A'],\n",
        "    ['RIGHT', 'B'],\n",
        "    ['B','RIGHT', 'A'],\n",
        "    ['LEFT', 'A'],\n",
        "    ['LEFT', 'B'],\n",
        "    ['B','LEFT', 'A'],\n",
        "    ['UP'],\n",
        "    ['DOWN']\n",
        "]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Smsgc42aHg5d"
      },
      "source": [
        "### Überführen der Textwerte in numerische\n",
        "Zu den definierten Textwerten, wird das dazugehörige numerische Array erzeugt. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gavje3Q05EFR",
        "outputId": "14fff8bf-e199-4b3b-bb43-064642d8b46b"
      },
      "source": [
        "action_values =  []\n",
        "for idx, action in enumerate(actions):\n",
        "  x = []\n",
        "  for key in action:\n",
        "    x = combine(x, single_actions[key])\n",
        "  action_values.append(x)\n",
        "action_values"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              " [1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
              " [1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
              " [1, 0, 0, 0, 0, 0, 0, 1, 1],\n",
              " [0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
              " [1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
              " [1, 0, 0, 0, 0, 0, 1, 0, 1],\n",
              " [0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUMwxo5bIS8z"
      },
      "source": [
        "### Schließen der Umgebung \n",
        "Da in Open-AI Gym nur jeweils eine Umgebung geöffnet sein kann, wird die Testumgebung, welche zum Ermitteln der Aktionswerte genutzt wurde, geschlossen. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GwxLpViNjOL"
      },
      "source": [
        "env.close()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRlmBfUWxrOX"
      },
      "source": [
        "## Super Mario Wrapper\n",
        "Da \"Super Mario Bros\" standardmäßig einen Action-Spec mit einer Form von (9,) hat, muss dieser um korrekt mit eine selbstgeschriebenen Wrapper transformiert werden. \n",
        "\n",
        "Um einen Umgebungs-Wrapper zu erstellen, sollte von der Klasse `PyEnvironmentBaseWrapper` abgeleitet werden. Die Klasse bietet bereits eine Standardimplementierung der benötigten Methoden eines Wrappers. In der Standardimplementierung der Methoden, wird lediglich die jeweilige Methode der gewrappeten Umgebung durchgereicht. Somit muss in den Wrappern nur noch die Funktionalität, die geändert werden soll, überschrieben werden. \n",
        "\n",
        "Der Wrapper `SuperMarioActionWrapper` ist ein Wrapper, der den Aktionsraum der Umgebung überschreibt. Dies ist notwendig, da der Agent `DqnAgent` aus der Bibliothek `TF-Agents` nicht mit einer mehrdimensionalen Aktionsspezifikation arbeiten kann. Der Konstruktor der Klasse, nimmt ein eindimensionales Array von Aktionen entgegen. Diese beinhalten die Aktionen, mit der später die `.step()` Methode angesprochen werden kann. Anschließend wird die Aktionsspezifikation überschrieben. Dazu wird eine Instanz der Klasse `BoundedArraySpec` erzeugt, welche die Form der Spezifikation, den Typ der Aktionen, sowie das Maximum und das Minimum des Aktionsraum, festlegt. \n",
        "\n",
        "Weiterhin werden die Methoden `action_spec()`, die die neue Aktionsspezifikation zurückliefert und `_step()`, welche gewählte Aktionen aus der Aktionsspezifikation zunächst auf eine Aktion der Umgebung überführt und anschließend einen Schritt ausführt, überschrieben."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OwN34nqxqb3"
      },
      "source": [
        "from tf_agents.environments import PyEnvironmentBaseWrapper\n",
        "\n",
        "class SuperMarioActionWrapper(PyEnvironmentBaseWrapper):\n",
        "\n",
        "  def __init__(self, env, actions=[]):\n",
        "    super(PyEnvironmentBaseWrapper, self).__init__()\n",
        "    self._env = env\n",
        "    self._actions = actions\n",
        "    self._action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), \n",
        "        dtype=np.int32, \n",
        "        minimum=0, \n",
        "        maximum=len(actions) - 1, \n",
        "        name='action'   \n",
        "    )\n",
        "      \n",
        "  # def observation_spec(self):\n",
        "  #   return self._observation_spec\n",
        "    \n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def _step(self, action):\n",
        "    return self._env.step(self._actions[action.item()])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o0BDknyIo7I"
      },
      "source": [
        "# Wrappen der Umgebungen\n",
        "Die Umgebung werden mit Hilfe verschiedener Klassen gewrappt. Dies hat den Vorteil, dass das Lernen beschleunigt und optimiert wird. Des Weiteren wird die Open-AI-Gym Retro Umgebung angepasst, sodass der Actionspace geflatted wird. Für den DQN-Algorithmus von TF-Agents ist ledeglich ein eindimensionaler Actionspace erlaubt. Der Wrapper FlattenActionWrapper transformiert den Actionspace des \"Super Mario\"-Spiels entsprechend."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17RIijvFxMOg"
      },
      "source": [
        "from tf_agents.environments.gym_wrapper import GymWrapper\n",
        "from gym.wrappers import FrameStack\n",
        "from tf_agents.environments import atari_wrappers\n",
        "from tf_agents.environments import wrappers\n",
        "from baselines.common import retro_wrappers\n",
        "import gym\n",
        "\n",
        "def wrap_env(env): \n",
        "  env = retro_wrappers.Downsample(env, 2)\n",
        "  env = retro_wrappers.Rgb2gray(env)\n",
        "  env = retro_wrappers.StartDoingRandomActionsWrapper(env, 100)\n",
        "  # env = gym.wrappers.GrayScaleObservation(env)\n",
        "  # env = gym.wrappers.ResizeObservation(env, (84,84))\n",
        "  # env = gym.wrappers.FlattenObservation(env)\n",
        "  env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "  env = gym.wrappers.FrameStack(env, 4)\n",
        "  env = GymWrapper(env)\n",
        "  env = SuperMarioActionWrapper(env, action_values)\n",
        "  env = wrappers.TimeLimit(env, 4_000)\n",
        "  env = wrappers.ActionRepeat(env, times=4)\n",
        "  return env"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z07Q0ZhK-fML",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de42742f-f2b7-4d43-c163-22845f15f1dc"
      },
      "source": [
        "from tf_agents.environments.gym_wrapper import GymWrapper\n",
        "from tf_agents.environments import wrappers\n",
        "from tf_agents.environments import utils\n",
        "\n",
        "game = retro.make(game='SuperMarioBros-Nes')\n",
        "\n",
        "train_py_env = wrap_env(game)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BoundedArraySpec(shape=(4, 112, 120, 1), dtype=dtype('uint8'), name='observation', minimum=0, maximum=255)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnNk17CdBBzL"
      },
      "source": [
        "Mit Hilfe der Funktion \"validate_py_environment\" kann sichergestellt werden, dass die Umgebung korrekt konfiguriert wurde und den definierten Spezifikation folgt. Der Aufurf nutzt eine Zufalls-Policy um Aktionen zun generieren. Insgesamt wird Umgebung einmal durchlaufen um zu ermitteln, dass die Konfiguration läuft wie gedacht. Wird ein \"time_step\" empfangen, der nicht zu Spezifikationen der Umgebung  passt wird ein Fehler geworfen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow4EfobnA9z1"
      },
      "source": [
        "utils.validate_py_environment(train_py_env, episodes=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnU2yjvJ4Ya0"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbokTtHsIja5"
      },
      "source": [
        "Umwandeln der Python-Umgebungen in Tensorflowumgebungen ([Quelle](https://www.tensorflow.org/agents/tutorials/2_environments_tutorial#tensorflow_environments))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBJpjzLo-fMM"
      },
      "source": [
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDPnXxB1qhoF",
        "outputId": "7186a849-68d0-438b-de5c-b83a52013f18"
      },
      "source": [
        "train_env"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf_agents.environments.tf_py_environment.TFPyEnvironment at 0x7f176a8627d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4G8NNUjOnIH"
      },
      "source": [
        "Erstellen eines Critic-Netzwerks, welches fürs Training genutzt wird."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjlzpJMRP9eu"
      },
      "source": [
        "Erstellen eines QNetworks, welches aus mehreren Schichten besteht und am ende für jede Action eine Ausgabe liefert. (Quelle: Buch S.654)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wENjkFxl52tF"
      },
      "source": [
        "from tf_agents.networks.q_network import QNetwork\n",
        "\n",
        "perprocessing_layer = tf.keras.layers.Lambda(\n",
        "   lambda obs: tf.cast(obs, np.float32) / 255.)\n",
        "\n",
        "conv_layer_params=[(32, (8, 8), 4), (64, (4, 4), 2), (64,(3, 3), 1)]\n",
        "fc_layer_params = [512]\n",
        "\n",
        "q_net = QNetwork(\n",
        "    train_env.observation_spec(),\n",
        "    train_env.action_spec(),\n",
        "    preprocessing_layers=perprocessing_layer,\n",
        "    conv_layer_params=conv_layer_params,\n",
        "    fc_layer_params=fc_layer_params\n",
        ")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-lCUSOTPdOh"
      },
      "source": [
        "Initialisieren des Agents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kXQ83aaPKNW",
        "outputId": "fc992d66-85de-4fdc-958c-23ff1928f947"
      },
      "source": [
        "# optimizer = tf.keras.optimizers.Adam(learning_rate=2-5e-4)\n",
        "\n",
        "optimizer = tf.keras.optimizers.RMSprop(lr=2.5e-4, rho=0.95, momentum=0.0, epsilon=0.00001, centered=True)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "epsilon_fn = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "    initial_learning_rate=1.0,\n",
        "    decay_steps=250_000,\n",
        "    end_learning_rate=0.01\n",
        ")\n",
        "\n",
        "agent = dqn_agent.DdqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    gamma=0.99,\n",
        "    train_step_counter=train_step_counter,\n",
        "    epsilon_greedy=lambda: epsilon_fn(train_step_counter)    \n",
        ")\n",
        "\n",
        "agent.initialize()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe5xqGbsPcKF"
      },
      "source": [
        "from tf_agents.replay_buffers.tf_uniform_replay_buffer import TFUniformReplayBuffer\n",
        "\n",
        "replay_buffer = TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=10_000\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoCvhBuuqZkP"
      },
      "source": [
        "replay_buffer_observer = replay_buffer.add_batch"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrNfXKOpqqim"
      },
      "source": [
        "## Metriken erstellen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GiHbPTaqpiH"
      },
      "source": [
        "from tf_agents.metrics import tf_metrics\n",
        "\n",
        "train_metrics = [\n",
        "      tf_metrics.NumberOfEpisodes(),\n",
        "      tf_metrics.EnvironmentSteps(),\n",
        "      tf_metrics.AverageReturnMetric(),\n",
        "      tf_metrics.AverageEpisodeLengthMetric()\n",
        "]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEIyEPAtt0n4"
      },
      "source": [
        "class ShowProgress:\n",
        "  def __init__(self, total):\n",
        "    self.counter = 0\n",
        "    self.total = total\n",
        "\n",
        "  def __call__(self, trajectory):\n",
        "    if not trajectory.is_boundary():\n",
        "      self.counter += 1\n",
        "    if self.counter % 100 == 0:\n",
        "      print(\"\\r{}/{}\".format(self.counter, self.total), end=\"\")"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJyAfZltsfr7"
      },
      "source": [
        "Der Stepdriver sammelt alle 4 Schritte die Erfahrungen ein und Schreibt diese in den Replay-Buffer. (Quelle: S.659)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HblyM7Lr7It"
      },
      "source": [
        "from tf_agents.drivers.dynamic_step_driver import  DynamicStepDriver\n",
        "\n",
        "collect_driver = DynamicStepDriver(\n",
        "    train_env,\n",
        "    agent.collect_policy,\n",
        "    observers=[replay_buffer_observer] + train_metrics,\n",
        "    num_steps=4\n",
        ")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoynSclIs8I2"
      },
      "source": [
        "### Vorbefüllen des Replaybuffers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiIchaADs70J"
      },
      "source": [
        "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
        "\n",
        "init_policy = RandomTFPolicy(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec()\n",
        ")\n",
        "\n",
        "init_driver = DynamicStepDriver(\n",
        "    train_env,\n",
        "    init_policy,\n",
        "    observers=[replay_buffer.add_batch, ShowProgress(20000)] + train_metrics,\n",
        "    num_steps=20000\n",
        ")"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu74ApGGwHis"
      },
      "source": [
        "## Erstellen eines Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Azk-0V69sy9M",
        "outputId": "e68ace85-fd13-489b-eff0-51e689171bd6"
      },
      "source": [
        "dataset = replay_buffer.as_dataset(\n",
        "    sample_batch_size=64,\n",
        "    num_steps=2,\n",
        "    num_parallel_calls=3, \n",
        "    single_deterministic_pass=False\n",
        ").prefetch(3)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py:382: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgt8U1mEw4PP"
      },
      "source": [
        "## Trainingsschleife erstellen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUxXmdRCw6PK"
      },
      "source": [
        "from tf_agents.utils.common import function\n",
        "\n",
        "collect_driver.run = function(collect_driver.run)\n",
        "agent.train = function(agent.train)\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sDN-Ww3xVlp"
      },
      "source": [
        "from tf_agents.eval.metric_utils import log_metrics\n",
        "\n",
        "def train_agent(n_iterations = 1000):\n",
        "  time_step = None\n",
        "  policy_state = agent.collect_policy.get_initial_state(train_env.batch_size)\n",
        "  iterator = iter(dataset)\n",
        "  for iteration in range(n_iterations):\n",
        "    time_step, policy_state = collect_driver.run(time_step, policy_state)\n",
        "    trajectories, buffer_info = next(iterator)\n",
        "    train_loss = agent.train(trajectories)\n",
        "    print(\"\\r{} loss:{:.5f}\".format(\n",
        "        iteration, train_loss.loss.numpy()), end=\"\"\n",
        "    )\n",
        "    if iteration % 100 == 0:\n",
        "      log_metrics(train_metrics)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnF2HVfQyxtq",
        "outputId": "0c2a6efe-f6ce-4f01-c170-099b4271a2c3"
      },
      "source": [
        "train_agent(200_000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tf_agents/drivers/dynamic_step_driver.py:206: calling while_loop_v2 (from tensorflow.python.ops.control_flow_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.while_loop(c, b, vars, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.while_loop(c, b, vars))\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
            "Instead of:\n",
            "results = tf.foldr(fn, elems, back_prop=False)\n",
            "Use:\n",
            "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
            "123788 loss:4538171904.00000"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVpg_kxiJL-D"
      },
      "source": [
        "train_env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blNPQ-HLZPsN"
      },
      "source": [
        "# Erstellen eines Videos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaGKqyN0ICNq"
      },
      "source": [
        "game = retro.make(game='SuperMarioBros-Nes')\n",
        "\n",
        "eval_py_env = wrap_env(game)\n",
        "\n",
        "# utils.validate_py_environment(eval_py_env, episodes=1)\n",
        "\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8VZ-rHzkqVZ"
      },
      "source": [
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-8fXoS6krOL"
      },
      "source": [
        "def create_policy_eval_video(policy, filename='imageio', num_episodes=5, fps=30):\n",
        "  video_filename = filename + '.mp4'\n",
        "  with imageio.get_writer(video_filename, fps=fps) as video:\n",
        "    for _ in range(num_episodes):\n",
        "      time_step = eval_env.reset()\n",
        "      video.append_data(eval_py_env.render())\n",
        "      while not time_step.is_last():\n",
        "        action_step = policy.action(time_step)\n",
        "        time_step = eval_env.step(action_step.action)\n",
        "        video.append_data(eval_py_env.render())\n",
        "  return embed_mp4(video_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-_baV8F0DF5"
      },
      "source": [
        "create_policy_eval_video(agent.policy, filename='first', num_episodes=3, fps=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaZI3ymd00rt"
      },
      "source": [
        "eval_env.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}